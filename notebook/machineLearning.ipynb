{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf7d6c6d",
   "metadata": {},
   "source": [
    "### The goal of this phase is to have your text classifier model ready to be used: that means, not only will you train it on a labeled dataset, but also you will take care of exporting it in a format suitable for later loading by the API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34a40d46",
   "metadata": {},
   "source": [
    "## Prepare the dataset for training\n",
    "\n",
    "You want to \"teach\" a machine to distinguish between spam and ham. Unfortunately, machines prefer to speak numbers rather than words. You then need to transform the human-readable CSV file above into a format that, albeit less readable by us puny humans, is more suited to the subsequent task of training the classifier. You will express (a cleaned-out version of) the text into a sequence of numbers, each representing a token (one word) forming the message text.\n",
    "\n",
    "More precisely:\n",
    "\n",
    "1. first you'll initialize a \"tokenizer\", asking it to build a dictionary (i.e. a token/number mapping) best suited for the texts at hand;\n",
    "2. then, you'll use the tokenizer to reduce all messages into (variable-length) sequences of numbers;\n",
    "3. these sequences will be \"padded\", i.e. you'll make sure they end up all having the same length: in this way, the whole dataset will be represented by a rectangular matrix of integer numbers, each row possibly having leading zeroes;\n",
    "4. the \"spam/ham\" column of the input dataset is recast with the \"*one-hot encoding*\": that is, it will become two columns, one for \"spamminess\" and one for \"hamminess\", both admitting the values zero or one (but with a single \"one\" per row): this turns out to be a formulation much friendlier to categorical classification tasks in general;\n",
    "5. finally you'll split the labeled dataset into a \"training\" and a \"testing\" disjoint parts. This is a very important concept: the effectiveness of a model should always be validated on data points *not used during training*.\n",
    "\n",
    "All these steps can be largely automated by using data-science Python packages such as `pandas`, `numpy`, `tensorflow/keras`.\n",
    "\n",
    "### Overview\n",
    "The dataset preparation starts with the CSV file you saw earlier and ends up exporting the new data format in the training/prepared_dataset directory. Two observations are in order:\n",
    "\n",
    "- the \"big matrix of numbers\" encoding the messages and the (narrower) one containing their spam/ham status are useless without the tokenizer: after all, to process a new message you would need to make it into a sequence of numbers using this very same mapping. For this reason, it is important to export the tokenizer as well, in order to later use the classifier.\n",
    "- the `pickle` protocol used in writing the reformulated data is strictly Python-specific and should not be treated as a long-term (or interoperable!) format. Later we discuss a sensible way to store model, tokenizer and metadata on disk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91485bcd",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35532b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "# Test for GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not configured to use a GPU. '\n",
    "      'Change this in Notebook Settings via the command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "    #raise SystemError('GPU device not found')\n",
    "\n",
    "# Disable GPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# List TF devices\n",
    "print(f\"Physical Devices: {tf.config.list_physical_devices()}\")\n",
    "print(f\"Logical Devices:  {tf.config.list_logical_devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input file\n",
    "datasetInputFile = '../training/dataset/spam-dataset.csv'\n",
    "# set the ouput file\n",
    "trainingDumpFile = '../training/prepared_dataset/spam_training_data.pickle'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f1ff672",
   "metadata": {},
   "source": [
    "### Reading and transforming the input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "630bbea3",
   "metadata": {},
   "source": [
    "#### Reading the input file and preparing legend info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets into a Pandas DataFrame\n",
    "df = pd.read_csv(datasetInputFile)\n",
    "\n",
    "# Convert Dataset to Lists\n",
    "labels = df['label'].tolist()\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Now we need to map our labels from being text values to being integer values. It's pretty simple:\n",
    "labelLegend = {'ham': 0, 'spam': 1}\n",
    "\n",
    "# The inverted legend is there to help us when we need to add a label to our predictions later.\n",
    "labelLegendInverted = {'%i' % v: k for k,v in labelLegend.items()}\n",
    "labelsAsInt = [labelLegend[x] for x in labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13813813",
   "metadata": {},
   "source": [
    "**Look at:** the contents of `texts`,\n",
    "`labelLegend`,\n",
    "`labelLegendInverted`,\n",
    "`labels`,\n",
    "`labelsAsInt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035316d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# texts\n",
    "# labelLegend\n",
    "# labelLegendInverted\n",
    "# labels\n",
    "# labelsAsInt\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4e525f5",
   "metadata": {},
   "source": [
    "#### Tokenization of texts\n",
    "The Keras Tokenizer will convert our raw text into vectors. Converting texts to vectors is a required step for any machine learning model (not just keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63703572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_NUM_WORDS is set to the current max length of any given post (tweet) on Twitter. This max number of words is likely to exceed *all* of our sms text size (typically 160 characters).\n",
    "MAX_NUM_WORDS = 280\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9b0bd32",
   "metadata": {},
   "source": [
    "**Look at:** `tokenizer.word_index`, `inverseWordIndex`, `sequences` and how they play together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed for demonstration purposes, will not be dumped with the rest:\n",
    "inverseWordIndex = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# tokenizer.word_index\n",
    "# inverseWordIndex\n",
    "# sequences\n",
    "# [[inverseWordIndex[i] for i in seq] for seq in sequences]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "960eb0e1",
   "metadata": {},
   "source": [
    "#### Create `X`, `y` training sets\n",
    "\n",
    "In machine learning, it's common to denote the training inputs as `X` and their corresponding labels (the outputs) as `y`. \n",
    "\n",
    "Let's start with the `X` data (aka the text) by padding all of our tokenized sequences. This ensures all training inputs are the same shape (aka size). \n",
    "\n",
    "Each sentence in each paragraph in every conversation you have is rarely the same length. It is almost certainly *sometimes* the same length, but rarely all the time. With that in mind, we want to categorize every sentence (or paragraph) as either `spam` or `ham` -- an arbitrary length of data into known length of data. \n",
    "\n",
    "This means we have two challenges:\n",
    "- Matrix multiplication has strict rules\n",
    "- Spoken or written language rarely adheres to strict rules.\n",
    "\n",
    "What to do?\n",
    "\n",
    "`X` as new representation for the `text` from our raw dataset. As stated above, there's a very small chance that all data in this group is the exact same length so we'll use the built-in tool called `pad_sequences` to correct for the inconsistent length. This length is actually called shape because of it's roots in linear algebra (matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b129bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 300\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6500c6b3",
   "metadata": {},
   "source": [
    "**Look at:** `sequences`, `X` and compare their shape and contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03801bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# [len(s) for s in sequences]\n",
    "# len(sequences)\n",
    "X.shape\n",
    "# type(X)\n",
    "# X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f1de341",
   "metadata": {},
   "source": [
    "#### Switch to categorical form for labels\n",
    "We convert our `labelsAsIntArray` into a corresponding matrix value (instead of just a list of ints) by using the built-in `to_categorical` function. The number of labels does not have to be 2 (as we have) but it should be at least 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsAsIntArray = np.asarray(labelsAsInt)\n",
    "y = to_categorical(labelsAsIntArray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9cb6486",
   "metadata": {},
   "source": [
    "**Look at:** `labelsAsIntArray`, `y` and how they relate to `labels` and `labelLegend`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# labelsAsIntArray\n",
    "# labelsAsIntArray.shape\n",
    "y.shape\n",
    "# y\n",
    "# labels\n",
    "# labelLegend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a4145e",
   "metadata": {},
   "source": [
    "#### Splitting the labeled dataset (train/test)\n",
    "\n",
    "If we trained on all of our data, our model will fit very *well* to that training data but it will not perform well on new data; aka it will be mostly useless.\n",
    "\n",
    "Since we have the `X` and `y` designations, we split the data into at least 2 corresponding sets: training data and validation data for each designation resulting in `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    "An easy way (but not the only way) is to use `scikit-learn` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c1e1d63",
   "metadata": {},
   "source": [
    "**Look at:** the shape of the four resulting numpy 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a001f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ec0d06",
   "metadata": {},
   "source": [
    "#### Save everything to file\n",
    "\n",
    "As we'll see soon, the test sets (aka `X_test` and `y_test`) are used to evaluate how our AI model is learning (aka the performance). This means it's often a good idea to save the test sets for future training and not splitting the data all over again. Using the same test set over and over will show how our model is performing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = {\n",
    "    'X_train': X_train, \n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'max_words': MAX_NUM_WORDS,\n",
    "    'max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'label_legend': labelLegend,\n",
    "    'label_legend_inverted': labelLegendInverted, \n",
    "    'tokenizer': tokenizer,\n",
    "}\n",
    "with open(trainingDumpFile, 'wb') as f:\n",
    "    pickle.dump(trainingData, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8be0950",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82be642c",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "It is time to train the model, i.e. fit a neural network to the task of associating a spam/ham label to a text message. Well, actually the task is now more like \"associating probabilities for 0/1 to a sequence of integer numbers (padded to fixed length with leading zeroes)\".\n",
    "\n",
    "The training  works as follows:\n",
    "\n",
    "1. All variables created and stored in the previous steps are loaded back to memory;\n",
    "2. A specific architecture of a neural network is created, still a \"blank slate\" in terms of what it \"knows\". Its core structure is that of a [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) (long-short-term-memory), a specific kind of recurrent neural network with some clever modifications aimed at enhancing its ability to \"remember\" things between non-adjacent locations in a sequence, such as two displaced positions in a string of text;\n",
    "3. The neural network (your classifier) is trained: that means it will progressively adapt its internal (many thousands of) parameters in order to best reproduce the input training set. Each individual neuron in the network is a relatively simple component - the \"intelligence\" coming from their sheer quantity and the particular choice of parameters determining which neurons affect which other and by how much;\n",
    "4. Once the training process has finished, the script carefully saves everything (model, tokenizer and associated metadata) in a format that can be later loaded by the API in a stand-alone way.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d7bc1c",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "#\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# in\n",
    "trainingDumpFile = '../training/prepared_dataset/spam_training_data.pickle'\n",
    "# out\n",
    "trainedModelFile = '../training/trained_model_v1/spam_model.h5'\n",
    "trainedMetadataFile = '../training/trained_model_v1/spam_metadata.json'\n",
    "trainedTokenizerFile = '../training/trained_model_v1/spam_tokenizer.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "567a0285",
   "metadata": {},
   "source": [
    "### Load the training data from previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data and extract its parts\n",
    "print('    Loading training data ... ', end ='')\n",
    "data = pickle.load(open(trainingDumpFile, 'rb'))\n",
    "X_test = data['X_test']\n",
    "X_train = data['X_train']\n",
    "y_test = data['y_test']\n",
    "y_train = data['y_train']\n",
    "labelLegendInverted = data['label_legend_inverted']\n",
    "labelLegend = data['label_legend']\n",
    "maxSeqLength = data['max_seq_length']\n",
    "maxNumWords = data['max_words']\n",
    "tokenizer = data['tokenizer']\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57eb1c6a",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda654d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model preparation\n",
    "embedDim = 128\n",
    "LstmOut = 196\n",
    "#\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxNumWords, embedDim, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LstmOut, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3395a50",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5d4e",
   "metadata": {},
   "source": [
    "#### Plotting Convergence\n",
    "\n",
    "During the training phase, we aim to minimize the error rate as well as to make sure that the model generalizes well on new data. Plotting loss (or error) vs. epoch and accuracy vs. epoch allows to visualise if we are underfitting or overfitting the model.\n",
    "\n",
    "Overfitting - (high variance) when the model fits perfectly to the training examples but has limited capability generalization.\n",
    "Underfitting - (high bias) if it didnâ€™t learn the data well enough.\n",
    "\n",
    "The training process should be stopped when the validation error trend changes from descending to ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3dc816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06fd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list = [PlotLearning()]\n",
    "\n",
    "print('    Training (it will take some minutes) ... ', end ='')\n",
    "batchSize = 32\n",
    "epochs = 3\n",
    "model.fit(X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            batch_size=batchSize, verbose=1,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "#            callbacks=callbacks_list\n",
    "            )\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d714873",
   "metadata": {},
   "source": [
    "### Export the Model, Metadata, and Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27e91885",
   "metadata": {},
   "source": [
    "#### Save the model proper (the model has its own format and its I/O methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    Saving model ... ', end ='')\n",
    "model.save(trainedModelFile)\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4d67f51",
   "metadata": {},
   "source": [
    "#### Save the metadata needed to 'run' the model as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    Saving metadata ... ', end ='')\n",
    "metadataForExport = {\n",
    "    'label_legend_inverted': labelLegendInverted,\n",
    "    'label_legend': labelLegend,\n",
    "    'max_seq_length': maxSeqLength,\n",
    "    'max_words': maxNumWords,\n",
    "}\n",
    "json.dump(metadataForExport, open(trainedMetadataFile, 'w'))\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db40f89c",
   "metadata": {},
   "source": [
    "### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d675c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    Saving tokenizer ... ', end ='')\n",
    "tokenizerJson = tokenizer.to_json()\n",
    "with open(trainedTokenizerFile, 'w') as f:\n",
    "    f.write(tokenizerJson)\n",
    "print('done')\n",
    "#\n",
    "print('FINISHED')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "954df683",
   "metadata": {},
   "source": [
    "Take a look in the output directory (`ls training/trained_model_v1`) to find:\n",
    "\n",
    "- A (small) JSON file with some metadata describing some features of the model;\n",
    "- A (larger) JSON file containing the full definition of the tokenizer. This has been created, and will be loaded, using helper functions provided with the tokenizer itself for our convenience;\n",
    "- A (rather large) binary file containing \"the model\". That means, among other things, the shape and topology of the neural network and all \"weights\", i.e. the parameters dictating which neurons will affect which others, and by how much. Saving and loading this file, which is in the HDF5 format, is best left to routines kindly offered by Keras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4336575",
   "metadata": {},
   "source": [
    "### Test the trained model\n",
    "\n",
    "Before moving on to the API section, make sure that the saved trained model is self-contained: that is, check that by loading the contents of `training/trained_model_v1`, and nothing else, you are able to perform meaningful estimates of the spam/ham status for a new arbitrary piece of text.\n",
    "\n",
    "Note that the output is given in terms of \"probabilities\", or \"confidence\". One can interpret a result like {'ham': 0.92, 'spam': 0.08} as meaning the input is ham with 92% confidence. Indeed, generally speaking, ML-based classifiers are very sophisticated and specialized machines for statistical inference.\n",
    "\n",
    "If you look at the (very simple) code of this function, you will see how the model, once loaded, is used to make predictions (it all boils down to the model's predict method, but first the input text must be recast as sequence of numbers with the aid of the tokenizer, and likewise the result must be made readable by humans again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e101dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and metadata:\n",
    "#   (in metadata, we'll need keys 'label_legend_inverted' and 'max_seq_length')\n",
    "tokenizer = tokenizer_from_json(open(trainedTokenizerFile).read())\n",
    "metadata = json.load(open(trainedMetadataFile))\n",
    "# Load the model:\n",
    "model = models.load_model(trainedModelFile)\n",
    "\n",
    "# a function for testing:\n",
    "def predictSpamStatus(text, spamModel, pMaxSequence, pLabelLegendInverted, pTokenizer):\n",
    "    sequences = pTokenizer.texts_to_sequences([text])\n",
    "    xInput = pad_sequences(sequences, maxlen=pMaxSequence)\n",
    "    yOutput = spamModel.predict(xInput)\n",
    "    preds = yOutput[0]\n",
    "    labeledPredictions = {pLabelLegendInverted[str(i)]: x for i, x in enumerate(preds)}\n",
    "    return labeledPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 'This is a nice touch, adding a sense of belonging and coziness. Thank you so much.'\n",
    "preds = predictSpamStatus(st, model, metadata['max_seq_length'], metadata['label_legend_inverted'], tokenizer)\n",
    "print('TEXT       = %s' % st)\n",
    "print('PREDICTION = %s' % str(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e41cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 'Click here to WIN A FREE IPHONE and this and that.'\n",
    "preds = predictSpamStatus(st, model, metadata['max_seq_length'], metadata['label_legend_inverted'], tokenizer)\n",
    "print('TEXT       = %s' % st)\n",
    "print('PREDICTION = %s' % str(preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88edeab0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
