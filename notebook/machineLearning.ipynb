{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf7d6c6d",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "The goal of this phase is to have your text classifier model ready to be used: that means, not only will you train it on a labeled dataset, but also you will take care of exporting it in a format suitable for later loading by the API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06a5b49e",
   "metadata": {},
   "source": [
    "## Inspect the starting dataset\n",
    "Open the file `training/dataset/spam-dataset.csv` and have a look at the lines there.\n",
    "> Tip: you can open a file in Gitpod by locating it with the \"File Explorer\" on your left, but if you like using the keyboard you may simply issue the command `gp open training/dataset/spam-dataset.csv` from the `bash` Console at the bottom.\n",
    "\n",
    "This is a CSV file with three columns (separated by commas):\n",
    "\n",
    "- whether the line is spam or \"ham\" (i.e. the opposite of spam),\n",
    "- a short piece of text (a \"message\"),\n",
    "- the tag identifying the source of this datapoint (this will be ignored by the scripts).\n",
    "\n",
    "\n",
    "The third column betrays the mixed origin of the data. To create our labeled dataset of 7,500 messages, two sets made available by the UCI Machine Learning Repository have been merged:\n",
    "- [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "- [YouTube Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)\n",
    "\n",
    "Luckily, the (not always fun) task of cleaning, validating and normalizing the heterogeneous (and usually imperfect) data has been already done for you -- something that is seldom the case, alas, in a real-world task.\n",
    "\n",
    "Look at line 352 of this file for example. Is that message spam or ham?\n",
    "> Tip: hit Ctrl-G in the Gitpod editor to jump to a specific line number.\n",
    "\n",
    "<details>\n",
    "<summary>Show me that line in Gitpod's editor</summary>\n",
    "<img src=\"../images/gitpod_gotoline.png\" />\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34a40d46",
   "metadata": {},
   "source": [
    "## Prepare the dataset for training\n",
    "\n",
    "You want to \"teach\" a machine to distinguish between spam and ham. Unfortunately, machines prefer to speak numbers rather than words. You then need to transform the human-readable CSV file above into a format that, albeit less readable by us puny humans, is more suited to the subsequent task of training the classifier. You will express (a cleaned-out version of) the text into a sequence of numbers, each representing a token (one word) forming the message text.\n",
    "\n",
    "More precisely:\n",
    "\n",
    "1. first you'll initialize a \"tokenizer\", asking it to build a dictionary (i.e. a token/number mapping) best suited for the texts at hand;\n",
    "2. then, you'll use the tokenizer to reduce all messages into (variable-length) sequences of numbers;\n",
    "3. these sequences will be \"padded\", i.e. you'll make sure they end up all having the same length: in this way, the whole dataset will be represented by a rectangular matrix of integer numbers, each row possibly having leading zeroes;\n",
    "4. the \"spam/ham\" column of the input dataset is recast with the \"*one-hot encoding*\": that is, it will become two columns, one for \"spamminess\" and one for \"hamminess\", both admitting the values zero or one (but with a single \"one\" per row): this turns out to be a formulation much friendlier to categorical classification tasks in general;\n",
    "5. finally you'll split the labeled dataset into a \"training\" and a \"testing\" disjoint parts. This is a very important concept: the effectiveness of a model should always be validated on data points *not used during training*.\n",
    "\n",
    "All these steps can be largely automated by using data-science Python packages such as `pandas`, `numpy`, `tensorflow/keras`.\n",
    "\n",
    "### Overview\n",
    "The dataset preparation starts with the CSV file you saw earlier and ends up exporting the new data format in the training/prepared_dataset directory. Two observations are in order:\n",
    "\n",
    "- the \"big matrix of numbers\" encoding the messages and the (narrower) one containing their spam/ham status are useless without the tokenizer: after all, to process a new message you would need to make it into a sequence of numbers using this very same mapping. For this reason, it is important to export the tokenizer as well, in order to later use the classifier.\n",
    "- the `pickle` protocol used in writing the reformulated data is strictly Python-specific and should not be treated as a long-term (or interoperable!) format. Later we discuss a sensible way to store model, tokenizer and metadata on disk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91485bcd",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input file\n",
    "datasetInputFile = '../training/dataset/spam-dataset.csv'\n",
    "# set the ouput file\n",
    "trainingDumpFile = '../training/prepared_dataset/spam_training_data.pickle'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f1ff672",
   "metadata": {},
   "source": [
    "### Reading and transforming the input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "630bbea3",
   "metadata": {},
   "source": [
    "#### Reading the input file and preparing legend info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets into a Pandas DataFrame\n",
    "df = pd.read_csv(datasetInputFile)\n",
    "\n",
    "# Convert Dataset to Lists\n",
    "labels = df['label'].tolist()\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Now we need to map our labels from being text values to being integer values. It's pretty simple:\n",
    "labelLegend = {'ham': 0, 'spam': 1}\n",
    "\n",
    "# The inverted legend is there to help us when we need to add a label to our predictions later.\n",
    "labelLegendInverted = {'%i' % v: k for k,v in labelLegend.items()}\n",
    "labelsAsInt = [labelLegend[x] for x in labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13813813",
   "metadata": {},
   "source": [
    "**Look at:** the contents of `texts`,\n",
    "`labelLegend`,\n",
    "`labelLegendInverted`,\n",
    "`labels`,\n",
    "`labelsAsInt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035316d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# texts\n",
    "# labelLegend\n",
    "# labelLegendInverted\n",
    "# labels\n",
    "# labelsAsInt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4e525f5",
   "metadata": {},
   "source": [
    "#### Tokenization of texts\n",
    "The Keras Tokenizer will convert our raw text into vectors. Converting texts to vectors is a required step for any machine learning model (not just keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63703572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_NUM_WORDS is set to the current max length of any given post (tweet) on Twitter. This max number of words is likely to exceed *all* of our sms text size (typically 160 characters).\n",
    "MAX_NUM_WORDS = 280\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9b0bd32",
   "metadata": {},
   "source": [
    "**Look at:** `tokenizer.word_index`, `inverseWordIndex`, `sequences` and how they play together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed for demonstration purposes, will not be dumped with the rest:\n",
    "inverseWordIndex = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# tokenizer.word_index\n",
    "# inverseWordIndex\n",
    "# sequences\n",
    "# [[inverseWordIndex[i] for i in seq] for seq in sequences]\n",
    "# texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "960eb0e1",
   "metadata": {},
   "source": [
    "#### Create `X`, `y` training sets\n",
    "\n",
    "In machine learning, it's common to denote the training inputs as `X` and their corresponding labels (the outputs) as `y`. \n",
    "\n",
    "Let's start with the `X` data (aka the text) by padding all of our tokenized sequences. This ensures all training inputs are the same shape (aka size). \n",
    "\n",
    "Each sentence in each paragraph in every conversation you have is rarely the same length. It is almost certainly *sometimes* the same length, but rarely all the time. With that in mind, we want to categorize every sentence (or paragraph) as either `spam` or `ham` -- an arbitrary length of data into known length of data. \n",
    "\n",
    "This means we have two challenges:\n",
    "- Matrix multiplication has strict rules\n",
    "- Spoken or written language rarely adheres to strict rules.\n",
    "\n",
    "What to do?\n",
    "\n",
    "`X` as new representation for the `text` from our raw dataset. As stated above, there's a very small chance that all data in this group is the exact same length so we'll use the built-in tool called `pad_sequences` to correct for the inconsistent length. This length is actually called shape because of it's roots in linear algebra (matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b129bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 300\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6500c6b3",
   "metadata": {},
   "source": [
    "**Look at:** `sequences`, `X` and compare their shape and contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03801bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# [len(s) for s in sequences]\n",
    "# len(sequences)\n",
    "# X.shape\n",
    "# type(X)\n",
    "# X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f1de341",
   "metadata": {},
   "source": [
    "#### Switch to categorical form for labels\n",
    "We convert our `labelsAsIntArray` into a corresponding matrix value (instead of just a list of ints) by using the built-in `to_categorical` function. The number of labels does not have to be 2 (as we have) but it should be at least 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsAsIntArray = np.asarray(labelsAsInt)\n",
    "y = to_categorical(labelsAsIntArray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9cb6486",
   "metadata": {},
   "source": [
    "**Look at:** `labelsAsIntArray`, `y` and how they relate to `labels` and `labelLegend`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# labelsAsIntArray\n",
    "# labelsAsIntArray.shape\n",
    "# y.shape\n",
    "# y\n",
    "# labels\n",
    "# labelLegend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a4145e",
   "metadata": {},
   "source": [
    "#### Splitting the labeled dataset (train/test)\n",
    "\n",
    "If we trained on all of our data, our model will fit very *well* to that training data but it will not perform well on new data; aka it will be mostly useless.\n",
    "\n",
    "Since we have the `X` and `y` designations, we split the data into at least 2 corresponding sets: training data and validation data for each designation resulting in `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    "An easy way (but not the only way) is to use `scikit-learn` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c1e1d63",
   "metadata": {},
   "source": [
    "**Look at:** the shape of the four resulting numpy 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a001f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment any one of the following and press Shift+Enter to print the variable\n",
    "# X_train.shape\n",
    "# X_test.shape\n",
    "# y_train.shape\n",
    "# y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ec0d06",
   "metadata": {},
   "source": [
    "#### Save everything to file\n",
    "\n",
    "As we'll see soon, the test sets (aka `X_test` and `y_test`) are used to evaluate how our AI model is learning (aka the performance). This means it's often a good idea to save the test sets for future training and not splitting the data all over again. Using the same test set over and over will show how our model is performing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = {\n",
    "    'X_train': X_train, \n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'max_words': MAX_NUM_WORDS,\n",
    "    'max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'label_legend': labelLegend,\n",
    "    'label_legend_inverted': labelLegendInverted, \n",
    "    'tokenizer': tokenizer,\n",
    "}\n",
    "with open(trainingDumpFile, 'wb') as f:\n",
    "    pickle.dump(trainingData, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
